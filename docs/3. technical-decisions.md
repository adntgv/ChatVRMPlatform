# Technical Decisions Document

## Architecture Overview

### Architectural Pattern: Modular Monolith → Microservices
- **Phase 1 (Current)**: Modular monolith with Next.js
  - Faster development and deployment
  - Simpler debugging and monitoring
  - Lower operational complexity
  
- **Phase 2 (Scale)**: Gradual extraction to microservices
  - Character rendering service
  - AI orchestration service
  - Voice synthesis service
  - User management service

**Rationale**: Start simple, extract services as bottlenecks emerge and team grows.

## Technology Stack

### Frontend
- **Framework**: Next.js 13+ (App Router)
  - **Why**: Full-stack capabilities, excellent DX, built-in optimizations
  - Server-side rendering for SEO
  - API routes for backend logic
  - Image optimization out of the box

- **3D Rendering**: Three.js + @pixiv/three-vrm
  - **Why**: Most mature WebGL library, official VRM support
  - Extensive ecosystem
  - Good performance on modern browsers
  
- **UI Framework**: Tailwind CSS + Radix UI
  - **Why**: Rapid development, accessible components
  - Type-safe with TypeScript
  - Consistent design system

- **State Management**: Zustand + React Query
  - **Why**: Lightweight, TypeScript-first
  - Zustand for client state
  - React Query for server state

### Backend
- **Runtime**: Node.js 18+ 
  - **Why**: JavaScript everywhere, huge ecosystem
  - Native fetch API
  - Better performance than previous versions

- **API Framework**: Next.js API Routes → Fastify (future)
  - **Why**: Start integrated, migrate when needed
  - Built-in TypeScript support
  - Easy middleware integration

- **Database**: PostgreSQL + Prisma ORM
  - **Why**: 
    - ACID compliance for financial data
    - JSON support for flexible schemas
    - Excellent performance at scale
    - Prisma provides type safety

- **Cache Layer**: Redis
  - **Why**: Session management, API response caching
  - Pub/sub for real-time features
  - Queue management

### Third-Party Services

#### Core AI Services
- **LLM Provider**: OpenAI (GPT-4)
  - **Why**: Best performance, streaming support
  - **Future**: Support for Anthropic, Google, open models

- **Voice Synthesis**: Koeiromap API
  - **Why**: High-quality Japanese voices, emotion support
  - **Future**: ElevenLabs, Azure Speech for multilingual

- **Knowledge Base**: Pinecone
  - **Why**: Purpose-built vector database
  - Scales to billions of embeddings
  - Low latency queries

#### Infrastructure Services
- **Authentication**: Auth0
  - **Why**: Enterprise-ready, social logins
  - MFA support out of the box
  - Compliance certifications

- **Payments**: Stripe
  - **Why**: Developer-friendly, global support
  - Subscription management
  - Usage-based billing

- **File Storage**: AWS S3 + CloudFront
  - **Why**: Reliable, cost-effective
  - Global CDN for VRM files
  - Presigned URLs for security

- **Monitoring**: Datadog
  - **Why**: Full-stack observability
  - Real user monitoring
  - AI/ML insights

## Deployment Strategy

### Hosting Platform: Vercel (Phase 1) → AWS ECS (Phase 2)

#### Phase 1: Vercel
- **Why**: 
  - Zero-config Next.js deployment
  - Automatic scaling
  - Edge functions for low latency
  - Integrated analytics

#### Phase 2: AWS ECS with Fargate
- **Why**:
  - More control over infrastructure
  - Cost optimization at scale
  - Multi-region deployment
  - Better resource allocation for 3D rendering

### CI/CD Pipeline
```yaml
Development → Staging → Production

Tools:
- GitHub Actions for CI
- Automated testing (Jest, Playwright)
- Security scanning (Snyk)
- Preview deployments for PRs
```

## Data Architecture

### Primary Database Schema
```sql
- users (id, email, auth0_id, created_at, subscription_tier)
- characters (id, user_id, name, config, status, created_at)
- vrm_models (id, name, file_url, category, approved)
- conversations (id, character_id, user_id, started_at)
- messages (id, conversation_id, content, role, created_at)
- knowledge_bases (id, character_id, pinecone_index)
- subscriptions (id, user_id, stripe_id, status, tier)
```

### Caching Strategy
- **Redis**: Session data, API responses (5min TTL)
- **CloudFront**: Static assets, VRM files
- **Browser**: Three.js assets, voice samples

## Security Decisions

### API Security
- JWT tokens with refresh rotation
- Rate limiting per user/IP
- API key encryption with AWS KMS
- CORS with whitelist

### Content Security
- Automated moderation with OpenAI
- Manual review queue
- VRM file scanning
- Prompt injection protection

## Scalability Considerations

### Horizontal Scaling Points
1. **Web servers**: Stateless, behind load balancer
2. **WebSocket servers**: Sticky sessions with Redis
3. **3D rendering**: WebGL on client, no server rendering
4. **AI requests**: Queue-based with rate limiting

### Performance Targets
- Page load: < 2s (P95)
- Character load: < 3s (P95)
- AI response: < 2s first token
- 99.9% uptime SLA

## Future Technical Considerations

### Mobile Support
- React Native app with react-three-fiber
- Optimized VRM models for mobile
- Offline mode with cached responses

### Internationalization
- Next.js i18n routing
- Multi-language voice synthesis
- RTL support
- Currency localization

### AI Model Flexibility
- LangChain for model abstraction
- Support for self-hosted models
- Fine-tuning pipeline
- Model performance monitoring